{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbFwQl5NQNa8OJjw/TGtHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariam688312/Project/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQyv9xt8f7_K",
        "outputId": "7a87a3a7-cb4c-4d41-8232-4fb40f544063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 527ms/step - accuracy: 0.3219 - loss: 1.3847 - val_accuracy: 0.3916 - val_loss: 1.1472\n",
            "Epoch 2/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 558ms/step - accuracy: 0.4726 - loss: 1.0370 - val_accuracy: 0.5633 - val_loss: 0.9382\n",
            "Epoch 3/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 545ms/step - accuracy: 0.5602 - loss: 0.8956 - val_accuracy: 0.7876 - val_loss: 0.5644\n",
            "Epoch 4/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 570ms/step - accuracy: 0.7552 - loss: 0.5783 - val_accuracy: 0.8259 - val_loss: 0.5024\n",
            "Epoch 5/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 561ms/step - accuracy: 0.8488 - loss: 0.4096 - val_accuracy: 0.8073 - val_loss: 0.5241\n",
            "Epoch 6/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 559ms/step - accuracy: 0.8593 - loss: 0.3814 - val_accuracy: 0.8341 - val_loss: 0.4814\n",
            "Epoch 7/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 562ms/step - accuracy: 0.8840 - loss: 0.3115 - val_accuracy: 0.8415 - val_loss: 0.4867\n",
            "Epoch 8/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 579ms/step - accuracy: 0.8875 - loss: 0.2975 - val_accuracy: 0.8381 - val_loss: 0.5256\n",
            "Epoch 9/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 560ms/step - accuracy: 0.8984 - loss: 0.2619 - val_accuracy: 0.8368 - val_loss: 0.4453\n",
            "Epoch 10/10\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 568ms/step - accuracy: 0.8863 - loss: 0.2978 - val_accuracy: 0.8646 - val_loss: 0.4130\n",
            "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 143ms/step - accuracy: 0.8688 - loss: 0.3861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8646\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import zipfile\n",
        "\n",
        "!wget -q -O UCI_HAR_Dataset.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
        "with zipfile.ZipFile(\"UCI_HAR_Dataset.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"UCI_HAR_Dataset\")\n",
        "\n",
        "base_path = \"./UCI_HAR_Dataset/UCI HAR Dataset/\"\n",
        "\n",
        "X_train = np.loadtxt(base_path + \"train/X_train.txt\")\n",
        "X_test = np.loadtxt(base_path + \"test/X_test.txt\")\n",
        "y_train = np.loadtxt(base_path + \"train/y_train.txt\", dtype=int)\n",
        "y_test = np.loadtxt(base_path + \"test/y_test.txt\", dtype=int)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = to_categorical(label_encoder.fit_transform(y_train))\n",
        "y_test = to_categorical(label_encoder.transform(y_test))\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 561, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 561, 1)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(561, 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "model.save(\"/kaggle/working/cnn_lstm_har.h5\")"
      ]
    }
  ]
}